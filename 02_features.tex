\chapter{Features in Streaming Anomaly Detection}
\label{c:features}

Before we delve into specific approaches developed for anomaly detection on data streams, we introduce a framework to compare such approaches. This section presents a foundational set of features that apply across all the categories of algorithms later described. We make use of these features to categorize and compare algorithms in \cref{c:approaches}.

The anomaly detection approaches we focus on work on sequences (or streams), which are ordered series of events of varying data type and dimensionality. The events in a sequence can be discrete (binary or part of a predefined alphabet), continuous (e.g., floating numbers), or multivariate. For batch datasets without an intrinsic ordering, we could apply various of the same approaches as well as many others \citep{hodge_survey_2004, chandola_anomaly_2009}. This work, however, focuses on anomaly detection for sequences/streams, which is also reflected in the features presented in this chapter.


\section{ANOM: What types of anomalies does an approach recognize?}

Anomalous observations in a sequence are often also referred to as spatial or \emph{point anomalies}, while anomalous (sub-)sequences that are anomalous in entirety are called \emph{collective anomalies}. Another definition of collective anomaly would be that an observation or (sub-)sequence is only anomalous in the context of multiple datasets or data streams \citep{chandola_anomaly_2012}. When a dataset is very sparse, it can be hard to detect anomalies reliably, as anomaly detectors will have difficulties deriving useful patterns or distributions. However, when multiple sparse datasets are evaluated in conjunction, a collective anomaly detector could find more anomalies \citep{zheng_detecting_2015}. An additional definition of anomaly, so-called \emph{contextual anomalies}, are events that are anomalous in one particular context but not necessarily in another one \citep{chandola_anomaly_2012}. According to \citet{hayes_contextual_2015}, the context consists of descriptive attributes in the dataset that could potentially explain anomalous behavior or reinforce the conclusion (e.g., location, time, or relationships).

Anomaly detection approaches generally focus on detecting a single one of three types of anomalies: a single anomalous event in a sequence (further referred to as ``Point''), an anomalous subsequence when compared to the remainder of the sequence (i.e., a ``discord'' as defined by \citet{keogh_hot_2005}, further referred to as ``Collective''), or a sequence that is anomalous in its entirety when compared to other sequences (further referred to as ``Sequence''). The length of an anomaly (from a single event to a subsequence of arbitrary length) is often not fixed, not known beforehand, and significantly varies across application domains, which poses a significant challenge for anomaly detection algorithms of all kinds \citep{chandola_anomaly_2012}.


\section{TRAIN: How is an approach trained?}
There are three different ways in which anomaly detection algorithms, as well as machine learning algorithms in general, can be trained: supervised learning, unsupervised learning, and semi-supervised learning. In supervised learning, an algorithm learns patterns in a dataset using a set of labeled observations. Once a supervised algorithm has been trained, it can assign labels to previously unseen data. While there are approaches to label data automatically, labeling is often performed in manual labor, making supervised algorithms expensive to develop and maintain. Supervised algorithms are often not directly applicable to real-time anomaly detection, as the streaming context requires continuous learning without manual interventions \citep{ahmad_unsupervised_2017}.

In unsupervised learning, algorithms are trained on an unlabeled dataset, based on which they learn patterns on their own. Unsupervised algorithms work under the assumption that data contains mostly normal data with only a few anomalies, which is reasonable as it is typically possible to collect a lot of normal data \citep{schneider_expected_2016}.

Semi-supervised algorithms are a mixture of supervised and unsupervised algorithms, where the training dataset contains labels, but only for a single class or a selection of all classes. For example, an algorithm for spam detection could be trained on a set of valid email conversations. The algorithm would learn to classify based on the information it has been given as well as its view of patterns in the dataset. Finally, the algorithm could classify future emails as normal or not-normal, the latter of which would  \citep{wang_statistical_2013}.

A learning capability that can be especially important in an evolving streaming context is incremental learning, which indicates that the model (i.e., the prediction function) is continuously updated based on incoming observations \citep{kontaki_continuous_2011, haibo_he_incremental_2011}. The capability for incremental learning is often a requirement in practical situations, as many streams change over time and as it can be impractical to gather and store representative training data from a potentially infinite high-velocity data stream, e.g., in traffic monitoring \citep{schneider_expected_2016, haibo_he_incremental_2011}. Furthermore, incremental learning algorithms often incorporate sophisticated sampling, randomization, and approximation techniques \citep{gama_survey_2012}. 

Depending on the definition, an algorithm using incremental learning can either access past and current data to evolve its model, or it can only access the past model and the current data. As past data is often not persisted in stream processing systems, the latter is frequently a more realistic assumption \citep{haibo_he_incremental_2011}. While the semantics of incremental learning and forgetting are well-researched for predictive systems, that work is still largely missing in areas like data stream clustering \citep{gama_survey_2012}.


\section{PROC: How does an approach process observations?}
The most intuitive approach to process newly arriving observations in streaming systems is processing them as soon as they arrive (i.e., “online”), and either persisting or forgetting them after processing. Many application areas like credit card fraud prevention require online processing, as the time a transaction takes to go through is critical to the business goal. Most online processing systems are built to quickly provide an approximate answer that is correct with a high probability instead of requiring an exact answer at all times \citep{gama_survey_2012}. Some algorithms, such as many clustering-based ones, work in a partially online fashion: an initial batch processing phase constructs a baseline model, after which an online processing phase updates that model \citep{ahmad_unsupervised_2017}. 

Contrary to processing events online, traditional batch processing is based on the idea that the entire dataset is available in memory before training/processing starts, which makes it inappropriate for handling high-velocity high-volume systems. Batch processing cannot keep up with the continuous changes that occur in data streams \citep{gama_survey_2012}.

The most practical approach combines batch and online processing in that it processes events in small (rolling) batches. These batches consist of events that have been collected in time- or count-based windows applied to the data stream and are continuously updated. Such windows can, for example, collect all events that arrive within five subsequent minutes, after which all events are processed, and the window slides further in time. This approach enables algorithms to forget objects that are no longer active instead of having to persist them in memory \citep{kontaki_continuous_2011}.


\section{DIM: What input dimensionality can an approach work with?}

As previously motivated, data streams can be either simple univariate time-series or complex multivariate sequences with many content-related and contextual variables. In general, all anomaly detection approaches differ in the dimensionality they expect and are capable of handling: while most approaches work on multivariate data streams, some cannot process data with more than one variable.

Data in high-dimensional datasets are generally sparsely distributed, which diminishes the meaning of distance and statistical measures and is hard to handle for many algorithms, a problem that is also called the ``curse of high-dimensionality'' \citep{yoon_nets:_2019}. While there are many statistical and machine learning methods for anomaly detection, most are not practically applicable to high-dimensional, large-scale problems \citep{schneider_expected_2016}. Some anomaly detection approaches apply sophisticated feature selection or dimensionality reduction to reduce the issues presented by very high-dimensional input data.


\section{DRIFT: How does an approach account for concept drift?}
A key challenge in anomaly detection on temporal data and data streams is the idea of concept drift. \emph{Concept drift} describes the fact that the target concept and the distribution that generates incoming events can both change over time. For example, the spending behavior of customers changes over time, which means that a model trained on past data becomes inconsistent, requiring algorithms to update their model of what is ``normal'' behavior, and to forget outdated information. A sequence that is subject to concept drift is also often described as being generated by a non-stationary distribution \citep{tsymbal_problem_2004}.

There have been several categorizations of concept drift in previous research, the most basic one being the distinction between sudden and gradual concept drift. While a \emph{sudden drift} occurs at a single point in time, \emph{gradual drift} spans a more extended period. For example, a production machine could gradually deteriorate, resulting in a decrease in the quality of its outputs. There have also been distinctions in the speed of gradual drifts, categorizing them into moderate and slow concept drift \citep{tsymbal_problem_2004}.

Additionally, the notion of \emph{virtual concept drift} has been introduced to describe a concept drift that only changes the generative distribution but leaves the target concept untouched \citep{tsymbal_problem_2004, siekmann_effective_1993}. In a more formal definition, virtual concept drift (or loose concept drift) is described as being a drift in which only the prior class probability changes, while in rigorous concept drift, the prior and conditional probabilities change \citep{bifet_new_2009}. In some cases, concepts might also drift in recurring patterns (e.g., if seasonality is involved). In such cases, algorithms could persist their model for later reuse once they detect and adapt to concept drift. However, the majority of learners does not yet account for recurring drift \citep{tsymbal_problem_2004}.

There are various ways to account for concept drift in an algorithm: by using only a window of the most recent instances to train the ``normal'' model, by weighting instances with their age to enable gradual forgetting of old events, or by using ensemble learning and applying aging to members of the ensemble, amongst others. Some of these approaches require specific machine learning techniques (e.g., algorithms that support weighting), while others are more generally applicable (e.g., windowing) \citep{tsymbal_problem_2004}. 

We can further distinguish between blind adaptation, which means using a static decay rate or window size (i.e., updating the model continuously independent of the occurrence of drift), or informed adaptation, which dynamically sets these parameters only when concept drift has been detected and thereby saves on computational efforts \citep{schneider_expected_2016, talagala_anomaly_2019}. Most algorithms regularly adapt without considering if there has even been a change. Contrarily, methods that apply explicit change detection only adapt if they detect a significant change (e.g., a so-called changepoint). Changepoints can be detected by monitoring performance metrics of the algorithm, as well as by monitoring how the distribution evolves across windows \citep{gama_survey_2012}.


\section{NOISE: How does the approach account for noise?}
While anomalies are defined as real events that do not conform to the expected notion of a ``normal'' event, noise describes events that stem from an entirely different distribution or a random process. Noise can be introduced by failing sensors, data transmission errors, user inputs, and many other failures. Noise is especially challenging in a streaming context, as noise removal algorithms often depend on having a view of the entire dataset (i.e., work on batches), which does not hold in the streaming context \citep{agrawal_adaptive_2017}.

Some algorithms integrate the handling of noise, which makes them more resilient and less prone to random errors. The main challenge in handling noise is how anomalous events can be distinguished from events that are noise, as well as how concept drift and noise can be separated. If an algorithm is too sensitive, it might classify noise as concept drift and update its model, while it might adapt to changes very slowly if it is highly resilient regarding noise \citep{tsymbal_problem_2004}. According to \citet{widmer_gerhard_learning_1996, tsymbal_problem_2004}, ``an ideal learner should combine robustness to noise and sensitivity to concept drift.''


\section{EV1-4: How was the approach evaluated?}
The methods chosen to evaluate anomaly detection algorithms are a vital part of any reliable and reproducible analysis. Therefore, we propose that a common feature for comparison should be dedicated to how an approach was evaluated, as well as how the evaluation procedure and artifacts are documented.

This section introduces four components that we deem critical for any evaluation for anomaly detection approaches. While our further analysis focuses on these four components, it is important to note that there can be many other factors depending on the domain.

\paragraph{EV1: Choice of Evaluation Measures.}
As the percentage of anomalies in a dataset is, by definition, extremely low, the commonly used false positive and true negative rates are not very meaningful in separation. Instead, evaluations should be based on more significant measures like precision and recall, or their combination, the F1-measure \citep{dos_santos_teixeira_data_2010}. Another combination of measures results in the Receiver Operating Characteristic (ROC) curve that is often used to evaluate the tradeoff between the detection rate and the false alarm rate \citep{pimentel_review_2014}.

A well-balanced anomaly detection algorithm should strive for high accuracy with a low number of false alarms. Furthermore, using commonly used measures for evaluation like precision and recall might not suffice in all cases, as they do not give weight to early detection of anomalies (i.e., it should be rewarded if algorithms detect an anomaly early rather than late) \citep{lavin_evaluating_2015}. In addition to measures evaluating accuracy, an approach can also be evaluated for its efficiency in processing based on time and memory requirements. 

\paragraph{EV2: Choice of Evaluation Datasets.}
A critical issue in comparing anomaly detection algorithms for data streams is the data and framework using which they are compared. Most datasets that were specifically designed for performance evaluations have been designed for static evaluations with fixed training and test datasets \citep{ahmad_unsupervised_2017, lavin_evaluating_2015}. There are only a few datasets freely available for streaming algorithm benchmarking, and many of them do not include concept drift and other challenges that occur in real-world applications. To evaluate specific issues like concept drift, it can often be useful to generate datasets artificially, as real-world datasets can contain concept drift, but do so in places that we do not know of \citep{bifet_new_2009}. 

Previous work has also introduced more general guidelines for the evaluation of streaming algorithms: one should either evaluate algorithms periodically using a holdout set of the data (that needs to evolve similarly to the stream) or by applying a performance measure as soon as new observations arrive (prequential). The holdout method is advantageous as it allows for usage of a balanced test set (with the same number of anomalies as normal observations), which is not the case in a real stream with prequential benchmarking \citep{schneider_expected_2016}.

\paragraph{EV3: Availability of Data and EV4: Availability of Algorithm.}
Having the ability to reproduce an approach based on a concise but complete algorithmic description and being able to evaluate an approach on the same data as used in the original research is a critical part of the scientific review process. These features are used to distinguish research that provides access to all the data used in the evaluation, as well as an algorithmic description or pseudocode of the algorithms used by the approach.